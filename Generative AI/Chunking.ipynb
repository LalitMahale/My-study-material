{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e00e509",
   "metadata": {},
   "source": [
    "# Text chunking\n",
    "\n",
    "- **Chunking* : chunking is the process of divide large text into small chunks so that we can process used in the context windwos of the llm.\n",
    "\n",
    "- **Overlap :-** Overlap included the some portion _or_ part of the previous chunk \n",
    "\n",
    "### 1. Character chunking : \n",
    "**Method :-** CharacterTextSplitter\n",
    "\n",
    "**Working :-** This method split the text into chunk based on character limit. it is a simple and streighforward way to divide text and ensuring that no chunk exceeds a certain number of character.\n",
    "\n",
    "\n",
    "### 2. Recursive chunking : \n",
    "**Method :-** RecursiveCharacterTextSplitter\n",
    "\n",
    "**Working :-** This is more advanced form of text splitting. it recursively tries to splite text besed on different delimiters like **paragraphs , sentenses, words** until the chunk size is not enough to fit withing the chunk size limit. it ensure a more natural and meaningful splits compared to splitted just by characters.\n",
    "\n",
    "- **It tries to maintain logical splits eg paragraph --> sentence --> word) rather than breaking sentence in between.**\n",
    "\n",
    "### 3. Sentence-based chunking : \n",
    "**Method :-** SentenceTextSplitter\n",
    "\n",
    "**Working :-** This splitter split text based on sentence bounderies. it ensures that the chunks don't break sentences in the middle, and maintaing logical cotext.\n",
    "\n",
    "- **Avoids the problem of cutting sentences in chunks which is critical when we need to preserve meaning.**\n",
    "\n",
    "-\n",
    "\n",
    "- Works well for **structured content** like **article, reports and blogs posts**\n",
    "\n",
    "\n",
    "### 4. Token -based chunking : \n",
    "**Method :-** TokenTextSplitter\n",
    "\n",
    "**Working :-** This method splits text into chunks based on the number of tokens. LLM like GPT-3 or GPT-4 works on tokens rather than characters, making this method extremely useful when you need respect the token limit of the model.\n",
    "\n",
    "- The most model-friendly splitting method, ensuring that the number of tokens in each chunk doesn't exceed the model's token limit.\n",
    "- Prevents token overflow by adjusting chunk sizes based on the actual token count rather than character count.\n",
    "- Works well with embeddings or any token-based model inputs.\n",
    "\n",
    "\n",
    "### 5. List -based chunking : \n",
    "**Method :-** ListTextSplitter\n",
    "\n",
    "**Working :-** This method design for structured data, specifically list of the strings. it allows to split long list of text entries into smaller groups or chunks \n",
    "\n",
    "- Useful for the dataset or any text which is already organized in list form eg **Bullet point, Question answer pairs**\n",
    "\n",
    "### 6. Custom chunking : \n",
    "**Method :-** CutomTextSplitter\n",
    "#\n",
    "**Working :-** This method allows us to define our won chunking logic. You can customize the splitting criteria based on any rule or structed you need eg **Based on Pattern, Reg x , domain specific rules.**\n",
    "#\n",
    "- this method provide full control to do chunks\n",
    "- mostly useful for domain specific chunking rules are requrieds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52484a9b",
   "metadata": {},
   "source": [
    "## Challenges in Chunkings\n",
    " \n",
    "- **1. Choosing the right chunking size :** if we choose too much small chunks we can loose the context _or_ meaning and if we choose too large chunks can exceed context windwos limit.\n",
    "    - **solution** : use recursive text splitter so that it will split by paragraph then sentence where we have to  adjusts the chunk size based on the documents structure.\n",
    "#        \n",
    "- **2. Handling incomplete sentence :** if chunk split in the middle of a sentence, it can confuse model and lead to poor understanding of the text.\n",
    "\n",
    "    - **solution** : Instead of splitting text based on characters or tokens use chunking method that splits at sentence bounderis. \n",
    "    \n",
    "      this methods will ensure that no sentence is cut in half\n",
    "#\n",
    "- **3. Loss of context :** When we splitting text, important information can be split across chunks, and it making hard for model to fully understand the content.\n",
    "\n",
    "    - **solution**:  use overlaping so that we won't loose more context it will repeat the text but rather than loosing context we can keep repeating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994f15cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Langchain provides various methods to chunk text for processing.']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"Langchain provides various methods to chunk text for processing.\"\n",
    "splitter = CharacterTextSplitter(chunk_size=20, chunk_overlap=5)\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)\n",
    "# Output: ['Langchain provides ', 'provides various me', 'ious methods to ch', 'ods to chunk text ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724f288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
